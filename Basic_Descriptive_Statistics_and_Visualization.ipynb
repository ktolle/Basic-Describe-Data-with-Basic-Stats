{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Descriptive Statistics with a Pandas Dataframes\n",
    "For this notebook we will use the Wisconsin Breast Cancer dataset (https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data). This is a dataset with an ID, Diagnosis ('M' for malignant and 'B' for benign) plus 30 features. \n",
    "\n",
    "You'll note that I use imports as I go so that you can see them where they get used--rather than piling them all up at the top of the file. This also means you only load them as you need them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# read in the file from UCI <recommend you save locally and load it if your connectivity is iffy>\n",
    "\n",
    "# Loading the file over the internet\n",
    "filename = \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\" \n",
    "\n",
    "# Loading the file locally in the same folder as the Python Notebook\n",
    "#filename = \"wi_breast_cancer.csv\"\n",
    "names = ['ID','Diagnosis','Mean-Radius','Mean-Texture','Mean-Perimeter','Mean-Area','Mean-Smoothness',\n",
    "         'Mean-Compactness','Mean-Concavity','Mean-ConcavePoints','Mean-Symmetry','Mean-FractalDimension',\n",
    "         'StdErr-Radius','StdErr-Texture','StdErr-Perimeter','StdErr-Area','StdErr-Smoothness',\n",
    "         'StdErr-Compactness','StdErr-Concavity','StdErr-ConcavePoints','StdErr-Symmetry','StdErr-FractalDimension',\n",
    "         'Worst-Radius','Worst-Texture','Worst-Perimeter','Worst-Area','Worst-Smoothness',\n",
    "         'Worst-Compactness','Worst-Concavity','Worst-ConcavePoints','Worst-Symmetry','Worst-FractalDimension']\n",
    "\n",
    "#loading the file into a dataframe\n",
    "data = pd.read_csv(filename, names=names, header=None) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start this lesson with a few ways to look at the data, such as the shape, info and description which are function built into Pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Group by Diagnosis \\n\", data.groupby('Diagnosis').size())  # how many in each class?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the \"Class\" to a Numeric or Boolean\n",
    "As we can see above, Diagnosis, the independent variable, is a categorical variable. Since it is a non-numeric value we will not be able see it using many of the built-in dataframe tools unless we convert it to a boolean or numeric value.\n",
    "\n",
    "We will: \n",
    "* Look at a section of the dataset where the value varies so that we can make sure we get what we want. \n",
    "* Map it to the numeric value we want 'M' or Malignant = 1 or True and 'B' or Benign = 0 or False. \n",
    "* Look at the same section to see if we were successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[\"Diagnosis\"][20:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Diagnosis to a numeric variable\n",
    "data['Diagnosis'] = data['Diagnosis'].map({'M': 1, 'B': 0})\n",
    "# Malignant tumors = 1 or True and Benign tumors = 0 or False\n",
    "print(data[\"Diagnosis\"][20:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Correlation \\n\" , data.corr(method='pearson'))             # how correlated are the features pairwise?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a high pairwise correlation across Mean, StdErr and Worst for these features, as you would expect. Probably we do not need to keep all of them--just the one of each that gives us the best outcome. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Skew \\n\", data.skew())                                     # how Gaussian are the features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sidebar, I also use pandas profiling (https://github.com/pandas-profiling/pandas-profiling) which generates a beautiful HTML page of the data. I'll let you check it out for yourselves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into dependent and independent variables\n",
    "Before we do anything, we should split our data into these classes and drop any variables that are not useful. ID appears to be the only non-useful variable for building our classification model. \n",
    "\n",
    "To create our classification model we need to assign the X and y. Diagnosis is our dependent variable ('y') and the remaining variables are our independent variables ('X'). We will drop 'ID' since we know it is non-predicting. \n",
    "\n",
    "In this section we will \"inform\" ourselves about the dataframe so we can properly segment it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, 2:32]   # load features into X dataframe\n",
    "y = data.iloc[:, 1]      # Load target into y dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that we have only the features we want\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that the y holds what we expect by looking at the same section of \"Diagnosis\" above\n",
    "y[20:25].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "Let's now take a look at the data distribution of the independent variables using matplotlib and various built in plots: \n",
    "\n",
    "#### Univariate\n",
    "* histograms\n",
    "* density\n",
    "* box\n",
    "* scatter\n",
    "\n",
    "#### Multivariate\n",
    "* Correlation matrix\n",
    "* Scatter matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms\n",
    "Histograms are a univariate (one variable at a time) plot that allows us to see the \"skew\" that was represented as a single value by dataframe.skew(). \n",
    "\n",
    "By binning the data we can see that many of the variables are skewed and some are nearly Gaussian. For example, StdErr-Area, which had the largest skew also has the most exponential looking distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# A helper for visualizing the data--sets the width and precision\n",
    "pd.set_option('display.width', 100) \n",
    "pd.set_option('precision', 3)\n",
    "\n",
    "# Look at the data distribution via histograms\n",
    "X.hist(figsize=(15,20), color = 'orange', edgecolor = 'blue')\n",
    "plt.subplots_adjust(wspace=.5, hspace=.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density Plots\n",
    "These univariate denisty plots are showing us the same information, but as a line rather than a histogram. Not clear why pyplot decided to show the identical information of a density and a histogram in a different order. May I suggest Seaborn? https://seaborn.pydata.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.plot(kind='density', subplots=True, layout=(6,5), sharex=False, figsize=(15,20))\n",
    "plt.subplots_adjust(wspace=.5, hspace=.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box Plots\n",
    "These plots also show the distribution, but add the quartile information. The box represents where 50% of the data can be found. The whisker lines show the 75% and the 25% percentiles. The circles are the datapoints that are potential data outliers. These fall 1.5 times larger than values inside the \"boxes\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.plot(kind='box', subplots=True, layout=(6,5), sharex=False, sharey=False, figsize=(15,20))\n",
    "plt.subplots_adjust(wspace=.5, hspace=.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatterplots \n",
    "NOTE: this is not terrifically interesting since Y is 0 or 1. I have a scatter matrix below that does a pairwise comparison, but with this many features, it's also hard to justify. \n",
    "\n",
    "PS: Was too lazy to put these into subplots. If I get around to honing my Seaborn skills I may update this code later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 2 \n",
    "\n",
    "for i in X:\n",
    "    plt.title(names[j])\n",
    "    plt.scatter(X, y, c=y)\n",
    "    plt.show()\n",
    "    j=j+1\n",
    "    \n",
    "j = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix\n",
    "With a smaller dataset one might find this interesting. As it is, with 30 features it is hard to see the value. Basically the zeroith row is the dependent variable--but telling which features are the yellowest is pretty tough. It's must better to do this via feature selection than to visualize the data when you have this many variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrix\n",
    "fig = plt.figure(figsize=(15,12))\n",
    "ax = fig.add_subplot(111) \n",
    "cax = ax.matshow(data[1:32].corr(), vmin=-1, vmax=1) \n",
    "fig.colorbar(cax) \n",
    "plt.subplots_adjust(wspace=.25, hspace=.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scattermatrix for a pairwise comparison\n",
    "As with Correlation Matrices, this works great for small datasets, but not well for features more than 10 as we have. I'm showing this for illustration purposes only. The purpose here, and with the correlation matrix above, is to show you multivariate plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "scatter_matrix(data, figsize=(15,20), c=y)\n",
    "plt.subplots_adjust(wspace=.01, hspace=.01)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
